from dotenv import load_dotenv
import os
import json
import yaml
import threading
import pickle
import uuid
import logging
import re

from langgraph.graph import END, StateGraph
from langchain_openai import ChatOpenAI
from langchain_google_vertexai import ChatVertexAI
from langchain.schema import HumanMessage, AIMessage
from typing import TypedDict, List, Optional

import prompts

# æ–°å¢: å°å…¥æ¨¡çµ„åŒ–èƒŒæ™¯å·¥å…·
import background_tool


logger = logging.getLogger(__name__)
# === Logging è¨­å®šï¼Œç¢ºä¿åœ¨ç›´æ¥åŸ·è¡Œæ™‚èƒ½è¼¸å‡ºåˆ° Terminal ===
if not logging.getLogger().handlers:  # æ ¹ logger ç„¡ handler æ™‚æ‰è¨­å®šï¼Œé¿å…é‡è¤‡
    logging.basicConfig(
        level=os.getenv("LOG_LEVEL", "INFO").upper(),
        format="%(asctime)s | %(levelname)-8s | %(threadName)s | %(name)s | %(message)s",
        datefmt="%H:%M:%S",
    )
# å€‹åˆ¥ logger å±¤ç´šï¼ˆå¯é€éç’°å¢ƒè®Šæ•¸èª¿æ•´ï¼‰
logger.setLevel(os.getenv("MODULE_LOG_LEVEL", "INFO").upper())

# è‹¥éœ€è¦é¡¯ç¤º langchain / httpx è©³ç´°å…§å®¹ï¼Œå¯è‡ªè¡Œè§£é™¤è¨»è§£
# logging.getLogger("langchain").setLevel("WARNING")
# logging.getLogger("httpx").setLevel("WARNING")


# --- JSON è§£æè¼”åŠ©ï¼šå®¹éŒ¯è™•ç†æ¨¡å‹è¼¸å‡ºå¤¾é›œæ–‡å­—æƒ…æ³ ---
def extract_first_json_list(text: str):
    """å˜—è©¦å¾æ–‡å­—ä¸­æ“·å–ç¬¬ä¸€å€‹ JSON list ä¸¦å›å‚³å…¶ç‰©ä»¶åˆ—è¡¨ã€‚å¤±æ•—å› []."""
    # å…ˆå˜—è©¦ç›´æ¥è§£æ
    try:
        data = json.loads(text)
        if isinstance(data, list):
            return data
    except Exception:
        pass
    # æ­£å‰‡å°‹æ‰¾ç¬¬ä¸€çµ„ [ ... ]
    match = re.search(r"\[[\s\S]*\]", text)
    if match:
        snippet = match.group(0)
        try:
            data = json.loads(snippet)
            if isinstance(data, list):
                return data
        except Exception:
            return []
    return []


# Define state
class AgentState(TypedDict):
    messages: List
    project_content: str  # ä¿®æ­£ç‚º str
    action_plan: str
    historical_log: str
    current_progress: str
    guidance_strategy: str
    score: str
    next_response: Optional[dict]
    session_id: str
    next_agent: Optional[str]
    stage_number: Optional[int]
    group_id: Optional[str]  # æ–°å¢çµ„åˆ¥ ID æ”¯æ´


# Load environment config
load_dotenv("./.env")

# Set Azure OpenAI API (æ”¯æ´åœ°ç«¯éƒ¨ç½²çš„ OpenAI æ¨¡å‹)
azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
azure_api_key = os.getenv("AZURE_OPENAI_API_KEY")
deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")

# Init LLM
if azure_endpoint and azure_api_key:
    # åœ°ç«¯ OpenAI æ¨¡å‹ä½¿ç”¨ ChatOpenAI
    # ç¢ºä¿ endpoint æœ‰ /v1 è·¯å¾‘
    endpoint = azure_endpoint
    if not endpoint.endswith("/v1"):
        endpoint = endpoint.rstrip("/") + "/v1"
    
    llm = ChatOpenAI(
        model=deployment_name,
        base_url=endpoint,
        api_key=azure_api_key,
        temperature=0,
        timeout=60,
        max_retries=2
    )
else:
    llm = ChatVertexAI(model_name="gemini-2.5-flash")

# Summary agent
# ä¾æ“šæ–°promptï¼Œéœ€å‚³éæ›´å¤šæ¬„ä½ï¼Œä¸¦è§£ææ–°æ ¼å¼


def build_current_progress(stage_number, messages, stage_settings):
    # å–å¾—å°æ‡‰éšæ®µè³‡è¨Š
    stage_key = f"stage_{stage_number}"
    stage = stage_settings.get(stage_key, {})
    stage_name = stage.get("name", f"éšæ®µ{stage_number}")
    score_list = stage.get("score_list", [])
    # å–æœ€è¿‘ä¸€å‰‡å°è©±æ‘˜è¦
    dialog_summary = messages[-1].content if messages else ""
    # çµ„åˆè©•åˆ†è¡¨æ ¼
    score_rows = "\n".join([f"| {item} | /5 |  |" for item in score_list])
    # çµ„åˆ current_progress
    current_progress = f"""
## ç•¶å‰ç‹€æ…‹èˆ‡è©•åˆ†

### ç•¶å‰éšæ®µ
- éšæ®µåç¨±ï¼š{stage_name}
- å°è©±æ‘˜è¦ï¼š{dialog_summary}

### è©•åˆ†é …ç›®

| è©•åˆ†é …ç›® | åˆ†æ•¸ | èªªæ˜ |
|----------|------|------|
{score_rows}
"""
    return current_progress


def summary_agent(state: AgentState) -> AgentState:
    logger.info(f"[summary_agent] state id: {id(state)}")
    logger.info(f"[summary_agent] state: {state}")
    prompt = prompts.SUMMARY_AGENT_PROMPT.format(
        current_dialog="\n".join([m.content for m in state["messages"][-3:]]),
        project_content=state.get("project_content", ""),
        action_plan=state.get("action_plan", ""),
        historical_log=state.get("historical_log", ""),
        current_progress=state.get("current_progress", ""),
    )
    input_tokens = count_tokens(prompt)
    TOKEN_STATS["summary_agent"]["input"] += input_tokens
    logger.info(
        f"[summary_agent] è¼¸å…¥ tokens: {input_tokens}, ç´¯è¨ˆè¼¸å…¥: {TOKEN_STATS['summary_agent']['input']}"
    )
    logger.info(f"ğŸ“ SummaryAgent è¼¸å…¥promptï¼š{prompt}")
    response = llm.invoke([HumanMessage(content=prompt)])
    output_tokens = count_tokens(response.content)
    TOKEN_STATS["summary_agent"]["output"] += output_tokens
    logger.info(
        f"[summary_agent] è¼¸å‡º tokens: {output_tokens}, ç´¯è¨ˆè¼¸å‡º: {TOKEN_STATS['summary_agent']['output']}"
    )
    logger.info(f"result(raw): {response.content}")
    parsed_list = extract_first_json_list(response.content)
    result = parsed_list[0] if parsed_list else {}
    with open("prompts/stage_setting.yaml", encoding="utf-8") as f:
        stage_settings = yaml.safe_load(f)
    prev_stage = state.get("stage_number", None)
    new_stage = result.get("stage_number")
    if isinstance(new_stage, str) and new_stage.isdigit():
        new_stage = int(new_stage)
    if new_stage is None:
        new_stage = prev_stage if prev_stage is not None else 1
    if new_stage != prev_stage or not state.get("current_progress"):
        state["current_progress"] = build_current_progress(
            new_stage, state["messages"], stage_settings
        )
    if result.get("project_content"):
        state["project_content"] = result["project_content"]
    if result.get("ACTION_PLAN"):
        state["action_plan"] = result["ACTION_PLAN"]
    if result.get("HISTORICAL_LOG"):
        state["historical_log"] = result["HISTORICAL_LOG"]
    state["stage_number"] = new_stage
    return state


# Score agent
# éœ€å‚³é action_plan, current_progress


def score_agent(state: AgentState) -> AgentState:
    logger.info(f"[score_agent] state id: {id(state)}")
    logger.info(f"[score_agent] state: {state}")
    prompt = prompts.SCORE_AGENT_PROMPT.format(
        current_dialog="\n".join([m.content for m in state["messages"][-3:]]),
        project_content=state.get("project_content", ""),
        action_plan=state.get("action_plan", ""),
        current_progress=state.get("current_progress", ""),
    )
    # è¨ˆç®—èˆ‡ç´¯è¨ˆ input token æ•¸é‡
    input_tokens = count_tokens(prompt)
    TOKEN_STATS["score_agent"]["input"] += input_tokens
    logger.info(
        f"[score_agent] è¼¸å…¥ tokens: {input_tokens}, ç´¯è¨ˆè¼¸å…¥: {TOKEN_STATS['score_agent']['input']}"
    )
    logger.info(f"ğŸ“ ScoreAgent è¼¸å…¥promptï¼š{prompt}")
    response = llm.invoke([HumanMessage(content=prompt)])
    logger.info(f"result(raw): {response.content}")
    # è¨ˆç®—èˆ‡ç´¯è¨ˆ output token æ•¸é‡
    output_tokens = count_tokens(response.content)
    TOKEN_STATS["score_agent"]["output"] += output_tokens
    logger.info(
        f"[score_agent] è¼¸å‡º tokens: {output_tokens}, ç´¯è¨ˆè¼¸å‡º: {TOKEN_STATS['score_agent']['output']}"
    )

    try:
        parsed_list = extract_first_json_list(response.content)
        result = parsed_list[0] if parsed_list else {}
    except Exception:
        result = {}
    state["current_progress"] = result.get(
        "current_progress", state.get("current_progress", "")
    )
    return state


# Decision agent
# éœ€å‚³éæ›´å¤šæ¬„ä½ï¼Œä¸¦è§£æ Guidance_and_Strategy


def decision_agent(state: AgentState) -> AgentState:
    logger.info(f"[decision_agent] state id: {id(state)}")
    logger.info(f"[decision_agent] state: {state}")
    prompt = prompts.DECISION_AGENT_PROMPT.format(
        current_dialog="\n".join([m.content for m in state["messages"][-3:]]),
        project_content=state.get("project_content", ""),
        action_plan=state.get("action_plan", ""),
        historical_log=state.get("historical_log", ""),
        current_progress=state.get("current_progress", ""),
    )
    _state_copy = state.copy()
    thread = threading.Thread(target=run_background_graph, args=(_state_copy,))
    thread.start()
    # è¨ˆç®—èˆ‡ç´¯è¨ˆ input token æ•¸é‡
    input_tokens = count_tokens(prompt)
    TOKEN_STATS["decision_agent"]["input"] += input_tokens
    logger.info(
        f"[decision_agent] è¼¸å…¥ tokens: {input_tokens}, ç´¯è¨ˆè¼¸å…¥: {TOKEN_STATS['decision_agent']['input']}"
    )
    logger.info(f"ğŸ“ DecisionAgent è¼¸å…¥promptï¼š{prompt}")
    response = llm.invoke([HumanMessage(content=prompt)])
    logger.info(f"result(raw): {response.content}")
    # è¨ˆç®—èˆ‡ç´¯è¨ˆ output token æ•¸é‡
    output_tokens = count_tokens(response.content)
    TOKEN_STATS["decision_agent"]["output"] += output_tokens
    logger.info(
        f"[decision_agent] è¼¸å‡º tokens: {output_tokens}, ç´¯è¨ˆè¼¸å‡º: {TOKEN_STATS['decision_agent']['output']}"
    )

    try:
        parsed_list = extract_first_json_list(response.content)
        result = parsed_list[0] if parsed_list else {}
    except Exception:
        result = {}
    state["guidance_strategy"] = result.get("Guidance_and_Strategy", "")
    return state


# PBL response agent
# éœ€å‚³é guidance_strategy


def response_agent(state: AgentState) -> AgentState:
    logger.info(f"[response_agent] state id: {id(state)}")
    logger.info(f"state: {state}")

    prompt = prompts.RESPONSE_AGENT_PROMPT.format(
        all_dialogs="\n".join([m.content for m in state["messages"][-10:]]),
        guidance_strategy=state.get("guidance_strategy", ""),
        project_content=state.get("project_content", ""),
        action_plan=state.get("action_plan", ""),
    )
    logger.info(f"ğŸ“ ResponseAgent è¼¸å…¥promptï¼š{prompt}")
    if "[CURRENT_PROJECT_CONTENT]" in prompt:
        current_project_content = state.get("project_content", "")
        prompt = prompt.replace("[CURRENT_PROJECT_CONTENT]", current_project_content)
    # è¨ˆç®—èˆ‡ç´¯è¨ˆ input token æ•¸é‡
    input_tokens = count_tokens(prompt)
    TOKEN_STATS["response_agent"]["input"] += input_tokens
    logger.info(
        f"[response_agent] è¼¸å…¥ tokens: {input_tokens}, ç´¯è¨ˆè¼¸å…¥: {TOKEN_STATS['response_agent']['input']}"
    )
    logger.info(f"ğŸ“ ResponseAgent è¼¸å…¥promptï¼š{prompt}")
    response = llm.invoke([HumanMessage(content=prompt)])
    # è¨ˆç®—èˆ‡ç´¯è¨ˆ output token æ•¸é‡
    output_tokens = count_tokens(response.content)
    TOKEN_STATS["response_agent"]["output"] += output_tokens
    logger.info(
        f"[response_agent] è¼¸å‡º tokens: {output_tokens}, ç´¯è¨ˆè¼¸å‡º: {TOKEN_STATS['response_agent']['output']}"
    )
    logger.info(f"ğŸ“ ResponseAgent å›è¦†ï¼š{response.content}")
    state["messages"].append(AIMessage(content=response.content))
    state["next_agent"] = None
    return state


# Workflow definition
# ä¸» workflowï¼š decision_agent å’Œ response_agent
main_graph_builder = StateGraph(AgentState)
main_graph_builder.add_node("decision_agent", decision_agent)
main_graph_builder.add_node("response_agent", response_agent)
main_graph_builder.set_entry_point("decision_agent")
main_graph_builder.add_edge("decision_agent", "response_agent")

main_graph = main_graph_builder.compile()

# èƒŒæ™¯ workflowï¼šsummary/score agent
background_graph_builder = StateGraph(AgentState)
background_graph_builder.add_node("summary_agent", summary_agent)
background_graph_builder.add_node("score_agent", score_agent)

background_graph_builder.set_entry_point("summary_agent")
background_graph_builder.add_edge("summary_agent", "score_agent")

background_graph = background_graph_builder.compile()

# å»ºç«‹ background_tool (æ¨¡çµ„åŒ–) ä¸¦è¨­å®š
background_tool.setup(background_graph, AIMessage, HumanMessage, logger=logger)


def run_background_graph(state):
    logger.info(f"[run_background_graph] state id: {id(state)}")
    session_id = state.get("session_id")
    group_id = state.get("group_id")
    
    for event in background_graph.stream(state):
        if isinstance(event, dict):
            state.update(event)
    
    if session_id:
        # å¦‚æœæœ‰ group_idï¼Œå„²å­˜åˆ°çµ„åˆ¥ç›®éŒ„
        if group_id:
            import os
            group_dir = os.path.join(os.getenv("GROUPS_DIR", "groups_data"), group_id)
            os.makedirs(group_dir, exist_ok=True)
            state_path = os.path.join(group_dir, f"state_{session_id}.pkl")
        else:
            # å¦å‰‡å„²å­˜åˆ°é è¨­ç›®éŒ„
            state_path = f"state_{session_id}.pkl"
        
        with open(state_path, "wb") as f:
            pickle.dump(state, f)
    
    logger.info("[Thread] èƒŒæ™¯ workflow ç‹€æ…‹å·²æ›´æ–°ï¼Œå·²å„²å­˜ state")


def run_graph(state):
    logger.info(f"[run_graph] state id: {id(state)}")
    logger.info(f"[run_graph] state: {state}")

    ai_reply = ""
    for event in main_graph.stream(state):
        if isinstance(event, dict):
            state.update(event)
        if "messages" in event:
            for msg in event["messages"]:
                if hasattr(msg, "type") and msg.type == "ai":
                    ai_reply = msg.content
    return ai_reply


# æ–°å¢ token çµ±è¨ˆèˆ‡è¨ˆæ•¸å‡½å¼
TOKEN_STATS = {
    "summary_agent": {"input": 0, "output": 0},
    "score_agent": {"input": 0, "output": 0},
    "decision_agent": {"input": 0, "output": 0},
    "response_agent": {"input": 0, "output": 0},
}


def count_tokens(text: str) -> int:
    try:
        import tiktoken

        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))
    except ImportError:
        logger.warning(
            "tiktoken library not installed, falling back to basic token count."
        )
        return len(text.split())


if __name__ == "__main__":
    from sample_buddy_G import HumanMessage, AIMessage

    def get_initial_state():
        initial_message = AIMessage(
            content="å—¨~ æˆ‘æ˜¯ä½ çš„ SDGs å°ˆæ¡ˆåŠ©ç†ã€‚è®“æˆ‘å€‘ä¸€èµ·æ¢ç´¢ä¸–ç•Œï¼Œäº†è§£ SDGsï¼Œç‚ºæˆ‘å€‘çš„åœ°çƒç›¡ä¸€ä»½å¿ƒå§ï¼"
        )
        return {
            "messages": [initial_message],
            "next_agent": None,
            "project_content": "",
            "action_plan": "",
            "historical_log": "",
            "current_progress": "",
            "guidance_strategy": "",
            "score": None,
            "next_response": None,
        }

    messages = [HumanMessage(content="æˆ‘æƒ³è§£æ±ºç¤¾å€çš„å‰©é£Ÿå•é¡Œã€‚")]
    session_id = str(uuid.uuid4())
    initial_state = {
        "messages": messages,
        "next_agent": None,
        "project_content": "",
        "action_plan": "",
        "historical_log": "",
        "current_progress": "",
        "guidance_strategy": "",
        "score": None,
        "next_response": None,
        "session_id": session_id,
    }
    for event in main_graph.stream(initial_state):
        for agent_state in event.values():
            if "messages" in agent_state:
                for msg in agent_state["messages"]:
                    print(f"{msg.__class__.__name__}: {msg.content}")
    while True:
        user_input = input("ä½ ï¼š")
        if user_input.lower() in {"exit", "quit", "çµæŸ"}:
            logger.info("âœ… å°è©±çµæŸã€‚")
            break
        messages.append(HumanMessage(content=user_input))
        initial_state["messages"] = messages
        ai_reply = run_graph(initial_state)
        if ai_reply:
            logger.info(f"AI: {ai_reply}")
