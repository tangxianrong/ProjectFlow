"""
æ¸¬è©¦åœ°ç«¯éƒ¨ç½² OpenAI æ¨¡å‹çš„ç°¡å–®ç¨‹å¼
"""
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

def test_local_openai():
    """
    æ¸¬è©¦åœ°ç«¯éƒ¨ç½²çš„ OpenAI æ¨¡å‹é€£ç·šèˆ‡åŸºæœ¬å°è©±åŠŸèƒ½
    """
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    load_dotenv()
    
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–åœ°ç«¯æ¨¡å‹è¨­å®š
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "")
    api_key = os.getenv("AZURE_OPENAI_API_KEY", "not-needed")
    model_name = os.getenv("AZURE_OPENAI_DEPLOYMENT", "")
    
    # ç¢ºä¿ endpoint æœ‰ /v1 è·¯å¾‘
    if not endpoint.endswith("/v1"):
        endpoint = endpoint.rstrip("/") + "/v1"
    
    print("=" * 50)
    print("ğŸ” åœ°ç«¯ OpenAI æ¨¡å‹é€£ç·šæ¸¬è©¦")
    print("=" * 50)
    print(f"Endpoint: {endpoint}")
    print(f"Model: {model_name}")
    print("=" * 50)
    
    try:
        # å»ºç«‹åœ°ç«¯ OpenAI å®¢æˆ¶ç«¯ï¼ˆä½¿ç”¨ ChatOpenAI è€Œé AzureChatOpenAIï¼‰
        llm = ChatOpenAI(
            model=model_name,
            base_url=endpoint,
            api_key=api_key,
            temperature=0.7,
            timeout=60,  # å¢åŠ è¶…æ™‚æ™‚é–“
            max_retries=2
        )
        
        print("\nâœ… æˆåŠŸå»ºç«‹åœ°ç«¯ OpenAI å®¢æˆ¶ç«¯\n")
        
        # æ¸¬è©¦ 1: ç°¡å–®å•ç­”
        print("ğŸ“ æ¸¬è©¦ 1: ç°¡å–®å•ç­”")
        print("-" * 50)
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€å€‹å‹å–„çš„AIåŠ©æ‰‹ã€‚"),
            HumanMessage(content="è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä½ è‡ªå·±ã€‚")
        ]
        
        response = llm.invoke(messages)
        print(f"å•é¡Œ: è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä½ è‡ªå·±ã€‚")
        print(f"å›ç­”: {response.content}")
        print("-" * 50)
        
        # æ¸¬è©¦ 2: æ•¸å­¸å•é¡Œ
        print("\nğŸ“ æ¸¬è©¦ 2: æ•¸å­¸å•é¡Œ")
        print("-" * 50)
        messages = [
            HumanMessage(content="123 + 456 ç­‰æ–¼å¤šå°‘ï¼Ÿè«‹ç›´æ¥å›ç­”æ•¸å­—ã€‚")
        ]
        
        response = llm.invoke(messages)
        print(f"å•é¡Œ: 123 + 456 ç­‰æ–¼å¤šå°‘ï¼Ÿ")
        print(f"å›ç­”: {response.content}")
        print("-" * 50)
        
        # æ¸¬è©¦ 3: ä¸²æµå›æ‡‰
        print("\nğŸ“ æ¸¬è©¦ 3: ä¸²æµå›æ‡‰")
        print("-" * 50)
        messages = [
            HumanMessage(content="è«‹èªªä¸€å€‹ç¬‘è©±ã€‚")
        ]
        
        print(f"å•é¡Œ: è«‹èªªä¸€å€‹ç¬‘è©±ã€‚")
        print(f"å›ç­”: ", end="")
        for chunk in llm.stream(messages):
            print(chunk.content, end="", flush=True)
        print("\n" + "-" * 50)
        
        print("\nâœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼\n")
        
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {str(e)}\n")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_local_openai()